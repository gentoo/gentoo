From 77fe140f8ad4d2fb266f1b49285479f6af25d6b7 Mon Sep 17 00:00:00 2001
From: devloop <nicolas.surribas@gmail.com>
Date: Wed, 16 Apr 2025 11:12:08 +0200
Subject: [PATCH] fix wappalyzer user warnings and deprecation warnings in
 other modules

--- a/wapitiCore/net/crawler.py
+++ b/wapitiCore/net/crawler.py
@@ -22,7 +22,7 @@
 from urllib.parse import urlparse, urlunparse
 import warnings
 import functools
-from typing import Dict
+from typing import Dict, Optional
 import asyncio
 import ssl
 
@@ -157,7 +157,7 @@ class AsyncCrawler:
             headers=headers,
             cookies=configuration.cookies,
             verify=ssl_context,
-            proxies=cls._proxy_url_to_dict(configuration.proxy),
+            proxy=cls._fix_proxy_url(configuration.proxy),
             timeout=configuration.timeout,
             event_hooks={"request": [drop_cookies_from_request]} if configuration.drop_cookies else None,
         )
@@ -166,10 +166,10 @@ class AsyncCrawler:
         return cls(configuration.base_request, client, configuration.timeout)
 
     @staticmethod
-    def _proxy_url_to_dict(proxy: str) -> Dict[str, str]:
+    def _fix_proxy_url(proxy: str) -> Optional[str]:
         """Set a proxy to use for HTTP requests."""
         if not proxy:
-            return {}
+            return None
 
         url_parts = urlparse(proxy)
         protocol = url_parts.scheme.lower()
@@ -180,10 +180,7 @@ class AsyncCrawler:
         if protocol == "socks":
             protocol = "socks5"
 
-        return {
-            "http://": urlunparse((protocol, url_parts.netloc, '/', '', '', '')),
-            "https://": urlunparse((protocol, url_parts.netloc, '/', '', '', '')),
-        }
+        return urlunparse((protocol, url_parts.netloc, '/', '', '', ''))
 
     @property
     def timeout(self):
